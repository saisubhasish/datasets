{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bca5ad",
   "metadata": {},
   "source": [
    "**1.\tIs it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c05b6",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "    \n",
    "The weights attached to the same neuron, continue to remain the same throughout the training. It makes the hidden units symmetric and this problem is known as the symmetry problem. Hence to break this symmetry the weights connected to the same neuron should not be initialized to the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dda3bc5",
   "metadata": {},
   "source": [
    "**2.\tIs it OK to initialize the bias terms to 0?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb934c",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "    \n",
    "It is important to note that setting biases to 0 will not create any problems as non-zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron will still be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b2676c",
   "metadata": {},
   "source": [
    "**3.\tName three advantages of the SELU activation function over ReLU.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821eb190",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "    \n",
    "- Like ReLU, SELU does not have vanishing gradient problem and hence, is used in deep neural networks.\n",
    "- Compared to ReLUs, SELUs cannot die.\n",
    "- SELUs learn faster and better than other activation functions without needing further procession."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa205d56",
   "metadata": {},
   "source": [
    "**4.\tIn which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f165e09",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "    \n",
    "- The **SELU** activation function is a good function by default.\n",
    "\n",
    "- If you need the neural network to be as fast as possible, you can use one of the **leaky ReLU** variants instead (e.g., a simple leaky ReLU using the default hyperparameter value).\n",
    "\n",
    "- Simplicity of the **ReLU** activation function makes it many people’s preferred option, despite the fact that it is generally outperformed by SELU and leaky ReLU. However, the ReLU activation function’s ability to output precisely zero can be useful in some cases (e.g., see Chapter 17). Moreover, it can sometimes benefit from optimized implementation as well as from hardware acceleration.\n",
    "\n",
    "- Hyperbolic tangent **(tanh)** can be useful in the output layer if you need to output a number between –1 and 1, but nowadays it is not used much in hidden layers (except in recurrent nets).\n",
    "\n",
    "- **Logistic activation** function is also useful in the output layer when you need to estimate a probability (e.g., for binary classification),rare in hidden layer\n",
    "\n",
    "- **Softmax**  we use this function at last layer of neural network which calculates the probabilities distribution of the event over ’n’ different events. The main advantage of the function is able to handle multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae710e",
   "metadata": {},
   "source": [
    "**5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc34835",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "    \n",
    "If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, lead you in the wrong direction (blow up and always go in the same direction), and/or oscillate around the global minima (making you jump too far)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600d256",
   "metadata": {},
   "source": [
    "**6.\tName three ways you can produce a sparse model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c24e4",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "    \n",
    "To produce a sparse model ways are :\n",
    "- regression.\n",
    "- machine-learning.\n",
    "- lasso.\n",
    "- regularization.\n",
    "- ridge-regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7346d",
   "metadata": {},
   "source": [
    "**7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfd11e",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "    \n",
    "Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed since it is only turned on during training. MC Dropout is exactly like dropout during training, but it is still active during inference, so each inference is slowed down slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de351a",
   "metadata": {},
   "source": [
    "**8.\tPractice training a deep neural network on the CIFAR10 image dataset:**\n",
    "\n",
    "    a.\tBuild a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "    b.\tUsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "    \n",
    "    c.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "    \n",
    "    d.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "    \n",
    "    e.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2d48f",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b4d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
